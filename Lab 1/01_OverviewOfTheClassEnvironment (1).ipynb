{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Overview of the Class Environment\n",
    "\n",
    "This notebook will introduce the basic knowledge of using AI clusters. You will have an overview of the Class Environment configured as an AI compute cluster. In addition, you will experiment with basic commands of the [SLURM cluster management](https://slurm.schedmd.com/overview.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "The goals of this notebook are to:\n",
    "* Understand the hardware configuration available for the class\n",
    "* Understand the basics commands for jobs submissions with SLURM\n",
    "* Run simple test scripts allocating different GPU resources\n",
    "* Connect interactively to a compute node and observe available resources\n",
    "\n",
    "**[1.1 The Hardware Configuration Overview](#1.1-The-Hardware-Configuration-Overview)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.1.1 Check The Available CPUs](#1.1.1-Check-The-Available-CPUs)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.1.2 Check the Available GPUs](#1.1.2-Check-The-Available-GPUs)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.1.3 Check The Interconnect Topology](#1.1.3-Check-The-Interconnect-Topology)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.1.4 Bandwidth & Connectivity Tests](#1.1.4-Bandwidth-and-Connectivity-Tests)<br>\n",
    "**[1.2 Basic SLURM Commands](#1.2-Basic-SLURM-Commands)<br>**\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.2.1 Check the SLURM Configuration](#1.2.1-Check-the-SLURM-Configuration)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.2.2 Submit Jobs Using SRUN Command](#1.2.2-Submit-jobs-using-SRUN-Command)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.2.3 Submit Jobs Using SBATCH Command](#1.2.3-Submit-jobs-using-SBATCH-Command])<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;[1.2.4 Exercise: Submit Jobs Using SBATCH Command Requesting More Resources](#1.2.4-Exercise-Submit-jobs-using-SBATCH-Command])<br>\n",
    "**[1.3 Run Interactive Sessions](#1.3-Run-Interactive-Sessions)<br>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.1 The Hardware Configuration Overview\n",
    "\n",
    "\n",
    "A modern AI cluster is a type of infrastructure designed for optimal Deep Learning model development. NVIDIA has designed DGXs servers as a full-stack solution for scalable AI development. Click the link to learn more about [DGX systems](https://www.nvidia.com/en-gb/data-center/dgx-systems/).\n",
    "\n",
    "Different deliveries of this course may have different hardware configurations. For benchmarking purposes, we will be using 4 A100s as a reference. This is about half the resources of a DGX 8xA100 server system (4 A100 GPUs, 4 NVlinks per GPU).\n",
    "\n",
    "<img  src=\"images/nvlink_v2.png\" width=\"600\"/>\n",
    "\n",
    "The hardware for this class has already been configured as a GPU cluster unit for Deep Learning. The cluster is organized as compute units (nodes) that can be allocated using a Cluster Manager (example SLURM). Among the hardware components, the cluster includes CPUs (Central Processing Units), GPUs (Graphics Processing Units), storage and networking.\n",
    "\n",
    "Let's look at the GPUs, CPUs and network design available in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.1 Check the Available CPUs \n",
    "\n",
    "We can check the CPU information of the system using the `lscpu` command. \n",
    "\n",
    "This example of outputs shows that there are 48 CPU cores of the `x86_64` from AMD.\n",
    "```\n",
    "Architecture:                    x86_64\n",
    "Core(s) per socket:              48\n",
    "Model name:                      AMD EPYC 7V13 64-Core Processor\n",
    "```\n",
    "For a complete description of the CPU processor architecture, check the `/proc/cpuinfo` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:                       x86_64\n",
      "CPU op-mode(s):                     32-bit, 64-bit\n",
      "Byte Order:                         Little Endian\n",
      "Address sizes:                      48 bits physical, 48 bits virtual\n",
      "CPU(s):                             96\n",
      "On-line CPU(s) list:                0-95\n",
      "Thread(s) per core:                 1\n",
      "Core(s) per socket:                 48\n",
      "Socket(s):                          2\n",
      "NUMA node(s):                       4\n",
      "Vendor ID:                          AuthenticAMD\n",
      "CPU family:                         25\n",
      "Model:                              1\n",
      "Model name:                         AMD EPYC 7V13 64-Core Processor\n",
      "Stepping:                           1\n",
      "CPU MHz:                            2445.434\n",
      "BogoMIPS:                           4890.86\n",
      "Hypervisor vendor:                  Microsoft\n",
      "Virtualization type:                full\n",
      "L1d cache:                          3 MiB\n",
      "L1i cache:                          3 MiB\n",
      "L2 cache:                           48 MiB\n",
      "L3 cache:                           384 MiB\n",
      "NUMA node0 CPU(s):                  0-23\n",
      "NUMA node1 CPU(s):                  24-47\n",
      "NUMA node2 CPU(s):                  48-71\n",
      "NUMA node3 CPU(s):                  72-95\n",
      "Vulnerability Gather data sampling: Not affected\n",
      "Vulnerability Itlb multihit:        Not affected\n",
      "Vulnerability L1tf:                 Not affected\n",
      "Vulnerability Mds:                  Not affected\n",
      "Vulnerability Meltdown:             Not affected\n",
      "Vulnerability Mmio stale data:      Not affected\n",
      "Vulnerability Retbleed:             Not affected\n",
      "Vulnerability Spec rstack overflow: Mitigation; safe RET, no microcode\n",
      "Vulnerability Spec store bypass:    Vulnerable\n",
      "Vulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __u\n",
      "                                    ser pointer sanitization\n",
      "Vulnerability Spectre v2:           Mitigation; Retpolines, STIBP disabled, RSB \n",
      "                                    filling, PBRSB-eIBRS Not affected\n",
      "Vulnerability Srbds:                Not affected\n",
      "Vulnerability Tsx async abort:      Not affected\n",
      "Flags:                              fpu vme de pse tsc msr pae mce cx8 apic sep \n",
      "                                    mtrr pge mca cmov pat pse36 clflush mmx fxsr\n",
      "                                     sse sse2 ht syscall nx mmxext fxsr_opt pdpe\n",
      "                                    1gb rdtscp lm constant_tsc rep_good nopl tsc\n",
      "                                    _reliable nonstop_tsc cpuid extd_apicid aper\n",
      "                                    fmperf pni pclmulqdq ssse3 fma cx16 pcid sse\n",
      "                                    4_1 sse4_2 movbe popcnt aes xsave avx f16c r\n",
      "                                    drand hypervisor lahf_lm cmp_legacy cr8_lega\n",
      "                                    cy abm sse4a misalignsse 3dnowprefetch osvw \n",
      "                                    topoext perfctr_core invpcid_single vmmcall \n",
      "                                    fsgsbase bmi1 avx2 smep bmi2 erms invpcid rd\n",
      "                                    seed adx smap clflushopt clwb sha_ni xsaveop\n",
      "                                    t xsavec xgetbv1 xsaves clzero xsaveerptr rd\n",
      "                                    pru arat umip vaes vpclmulqdq rdpid fsrm\n"
     ]
    }
   ],
   "source": [
    "# Display information CPUs\n",
    "!lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu cores\t: 48\n"
     ]
    }
   ],
   "source": [
    "# Check the number of CPU cores\n",
    "!grep 'cpu cores' /proc/cpuinfo | uniq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Check the Available  GPUs \n",
    "\n",
    "The NVIDIA System Management Interface `nvidia-smi` is a command for monitoring NVIDIA GPU devices. Several key details are listed such as the CUDA and  GPU driver versions, the number and type of GPUs available, the GPU memory each, running GPU process, etc.\n",
    "\n",
    "In the following example, `nvidia-smi` command shows that there are GPUs, each with approximately 80GB of memory. \n",
    "\n",
    "<img  src=\"images/nvidia_smi.png\" width=\"600\"/>\n",
    "\n",
    "For more details, refer to the [nvidia-smi documentation](https://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 21 14:31:15 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              55W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              54W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          On  | 00000003:00:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              54W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          On  | 00000004:00:00.0 Off |                    0 |\n",
      "| N/A   40C    P0              58W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Display information about GPUs\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3 Check the Available Interconnect Topology \n",
    "\n",
    "\n",
    "\n",
    "<img  align=\"right\" src=\"images/nvlink_nvidia.png\" width=\"420\"/>\n",
    "\n",
    "The multi-GPU system configuration needs a fast and scalable interconnect. [NVIDIA NVLink technology](https://www.nvidia.com/en-us/data-center/nvlink/) is a direct GPU-to-GPU interconnect providing high bandwidth and improving scalability for multi-GPU systems.\n",
    "\n",
    "To check the available interconnect topology, we can use `nvidia-smi topo --matrix` command. In this class, we should get 4 NVLinks per GPU device. \n",
    "\n",
    "```\n",
    "        GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity\n",
    "GPU0     X      NV12    SYS     SYS     0-23            N/A\n",
    "GPU1    NV12     X      SYS     SYS     24-47           N/A\n",
    "GPU2    SYS     SYS      X      NV12    48-71           N/A\n",
    "GPU3    SYS     SYS     NV12     X      72-95           N/A\n",
    "\n",
    "Where X= Self and NV# = Connection traversing a bonded set of # NVLinks\n",
    "```\n",
    "\n",
    "In this environment, notice only 1 link between GPU0 and GPU1, GPU2 while 2 links are shown between GPU0 and GPU3.\n",
    "\n",
    "---\n",
    "\n",
    "The image you've shared seems to be a snippet from a document describing the interconnect topology of a multi-GPU system, specifically referencing NVIDIA's NVLink technology. \"SYS\" in the context of the GPU interconnect topology usually stands for \"system interconnect.\" It means that the GPUs in question are connected via the system's standard PCIe interface rather than a high-speed NVLink connection.\n",
    "\n",
    "NVLink is an interconnect technology developed by NVIDIA that enables high bandwidth data transfer between GPUs and between GPUs and CPUs. It is designed to be much faster than the traditional PCIe bus found in most computers. \n",
    "\n",
    "The table in your image is showing the connectivity between four GPUs (GPU0 to GPU3). Here's how to interpret the connections:\n",
    "\n",
    "- GPU0 has one NVLink to GPU1 (NV12 signifies one connection that could represent a pair of NVLink connections as NVLink typically comes in pairs) and a system interconnect (SYS) to GPUs 2 and 3.\n",
    "- GPU1 is similarly connected with one NVLink to GPU0 and system interconnects to GPUs 2 and 3.\n",
    "- GPU2 has a system interconnect to GPUs 0 and 1 and one NVLink to GPU3.\n",
    "- GPU3 has a system interconnect to GPUs 0 and 1 and one NVLink to GPU2.\n",
    "\n",
    "Ideally, according to the document, each GPU should have four NVLink connections to the other GPUs, suggesting a fully connected topology where each GPU can communicate with every other GPU via NVLink. However, the topology described in the document indicates a less optimal connection, with some GPUs connected via NVLink and others via the slower system bus. This can have implications for the performance of tasks that require high-speed GPU-to-GPU communication, such as deep learning and complex simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\u001b[4mGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\n",
      "GPU0\t X \tNV12\tSYS\tSYS\t0-23\t0\t\tN/A\n",
      "GPU1\tNV12\t X \tSYS\tSYS\t24-47\t1\t\tN/A\n",
      "GPU2\tSYS\tSYS\t X \tNV12\t48-71\t2\t\tN/A\n",
      "GPU3\tSYS\tSYS\tNV12\t X \t72-95\t3\t\tN/A\n",
      "\n",
      "Legend:\n",
      "\n",
      "  X    = Self\n",
      "  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\n",
      "  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\n",
      "  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\n",
      "  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\n",
      "  PIX  = Connection traversing at most a single PCIe bridge\n",
      "  NV#  = Connection traversing a bonded set of # NVLinks\n"
     ]
    }
   ],
   "source": [
    "# Check Interconnect Topology \n",
    "!nvidia-smi topo --matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to check the NVLink status and bandwidth using `nvidia-smi nvlink --status` command. You should see similar outputs per device.\n",
    "```\n",
    "GPU 0: Graphics Device\n",
    "\t Link 0: 25 GB/s\n",
    "\t Link 1: 25 GB/s\n",
    "\t Link 2: 25 GB/s\n",
    "\t Link 3: 25 GB/s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA A100 80GB PCIe (UUID: GPU-faed01c7-25fe-0f09-5dee-1012b6f0b3c3)\n",
      "\t Link 0: 25 GB/s\n",
      "\t Link 1: 25 GB/s\n",
      "\t Link 2: 25 GB/s\n",
      "\t Link 3: 25 GB/s\n",
      "\t Link 4: 25 GB/s\n",
      "\t Link 5: 25 GB/s\n",
      "\t Link 6: 25 GB/s\n",
      "\t Link 7: 25 GB/s\n",
      "\t Link 8: 25 GB/s\n",
      "\t Link 9: 25 GB/s\n",
      "\t Link 10: 25 GB/s\n",
      "\t Link 11: 25 GB/s\n",
      "GPU 1: NVIDIA A100 80GB PCIe (UUID: GPU-f5a14843-d8a5-c0de-f673-0ede7190fdce)\n",
      "\t Link 0: 25 GB/s\n",
      "\t Link 1: 25 GB/s\n",
      "\t Link 2: 25 GB/s\n",
      "\t Link 3: 25 GB/s\n",
      "\t Link 4: 25 GB/s\n",
      "\t Link 5: 25 GB/s\n",
      "\t Link 6: 25 GB/s\n",
      "\t Link 7: 25 GB/s\n",
      "\t Link 8: 25 GB/s\n",
      "\t Link 9: 25 GB/s\n",
      "\t Link 10: 25 GB/s\n",
      "\t Link 11: 25 GB/s\n",
      "GPU 2: NVIDIA A100 80GB PCIe (UUID: GPU-0dfcb01d-0d50-c193-026b-a3ab60cff441)\n",
      "\t Link 0: 25 GB/s\n",
      "\t Link 1: 25 GB/s\n",
      "\t Link 2: 25 GB/s\n",
      "\t Link 3: 25 GB/s\n",
      "\t Link 4: 25 GB/s\n",
      "\t Link 5: 25 GB/s\n",
      "\t Link 6: 25 GB/s\n",
      "\t Link 7: 25 GB/s\n",
      "\t Link 8: 25 GB/s\n",
      "\t Link 9: 25 GB/s\n",
      "\t Link 10: 25 GB/s\n",
      "\t Link 11: 25 GB/s\n",
      "GPU 3: NVIDIA A100 80GB PCIe (UUID: GPU-eaef7178-19f7-0d0c-07ba-c428f755500a)\n",
      "\t Link 0: 25 GB/s\n",
      "\t Link 1: 25 GB/s\n",
      "\t Link 2: 25 GB/s\n",
      "\t Link 3: 25 GB/s\n",
      "\t Link 4: 25 GB/s\n",
      "\t Link 5: 25 GB/s\n",
      "\t Link 6: 25 GB/s\n",
      "\t Link 7: 25 GB/s\n",
      "\t Link 8: 25 GB/s\n",
      "\t Link 9: 25 GB/s\n",
      "\t Link 10: 25 GB/s\n",
      "\t Link 11: 25 GB/s\n"
     ]
    }
   ],
   "source": [
    "# Check nvlink status\n",
    "!nvidia-smi nvlink --status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.4 Bandwidth & Connectivity Tests\n",
    "\n",
    "\n",
    "NVIDIA provides an application **p2pBandwidthLatencyTest** that demonstrates CUDA Peer-To-Peer (P2P) data transfers between pairs of GPUs by computing bandwidth and latency while enabling and disabling NVLinks. This tool is part of the code samples for CUDA Developers [cuda-samples](https://github.com/NVIDIA/cuda-samples.git). \n",
    "\n",
    "Example outputs are shown below. Notice the Device to Device (D\\D) bandwidth differences when enabling and disabling NVLinks (P2P).\n",
    "\n",
    "```\n",
    "Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n",
    "   D\\D     0      1      2      3 \n",
    "     0 1529.61 516.36  20.75  21.54 \n",
    "     1 517.04 1525.88  20.63  21.33 \n",
    "     2  20.32  20.17 1532.61 517.23 \n",
    "     3  20.95  20.83 517.98 1532.61 \n",
    "\n",
    "Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
    "   D\\D     0      1      2      3 \n",
    "     0 1532.61  18.09  20.79  21.52 \n",
    "     1  18.11 1531.11  20.65  21.33 \n",
    "     2  20.32  20.17 1528.12  28.89 \n",
    "     3  20.97  20.82  28.36 1531.11 \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]\n",
      "Device: 0, NVIDIA A100 80GB PCIe, pciBusID: 0, pciDeviceID: 0, pciDomainID:1\n",
      "Device: 1, NVIDIA A100 80GB PCIe, pciBusID: 0, pciDeviceID: 0, pciDomainID:2\n",
      "Device: 2, NVIDIA A100 80GB PCIe, pciBusID: 0, pciDeviceID: 0, pciDomainID:3\n",
      "Device: 3, NVIDIA A100 80GB PCIe, pciBusID: 0, pciDeviceID: 0, pciDomainID:4\n",
      "Device=0 CAN Access Peer Device=1\n",
      "Device=0 CANNOT Access Peer Device=2\n",
      "Device=0 CANNOT Access Peer Device=3\n",
      "Device=1 CAN Access Peer Device=0\n",
      "Device=1 CANNOT Access Peer Device=2\n",
      "Device=1 CANNOT Access Peer Device=3\n",
      "Device=2 CANNOT Access Peer Device=0\n",
      "Device=2 CANNOT Access Peer Device=1\n",
      "Device=2 CAN Access Peer Device=3\n",
      "Device=3 CANNOT Access Peer Device=0\n",
      "Device=3 CANNOT Access Peer Device=1\n",
      "Device=3 CAN Access Peer Device=2\n",
      "\n",
      "***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.\n",
      "So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases.\n",
      "\n",
      "P2P Connectivity Matrix\n",
      "     D\\D     0     1     2     3\n",
      "     0\t     1     1     0     0\n",
      "     1\t     1     1     0     0\n",
      "     2\t     0     0     1     1\n",
      "     3\t     0     0     1     1\n",
      "Unidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
      "   D\\D     0      1      2      3 \n",
      "     0 1503.85  21.41  21.67  21.89 \n",
      "     1  21.37 1509.66  21.60  21.65 \n",
      "     2  21.55  21.42 1508.20  21.62 \n",
      "     3  21.52  21.44  21.89 1512.58 \n",
      "Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s)\n",
      "   D\\D     0      1      2      3 \n",
      "     0 1506.75 275.35  21.62  21.61 \n",
      "     1 275.52 1524.39  21.52  21.83 \n",
      "     2  21.49  21.37 1505.30 275.60 \n",
      "     3  21.49  21.54 275.25 1522.90 \n",
      "Bidirectional P2P=Disabled Bandwidth Matrix (GB/s)\n",
      "   D\\D     0      1      2      3 \n",
      "     0 1525.88  26.44  29.62  29.58 \n",
      "     1  25.96 1528.86  29.57  29.50 \n",
      "     2  27.48  27.55 1528.86  29.27 \n",
      "     3  27.51  27.88  29.43 1525.88 \n",
      "Bidirectional P2P=Enabled Bandwidth Matrix (GB/s)\n",
      "   D\\D     0      1      2      3 \n",
      "     0 1531.11 516.02  29.60  29.57 \n",
      "     1 515.85 1528.12  29.45  29.60 \n",
      "     2  27.54  27.85 1528.86 516.89 \n",
      "     3  27.41  27.90 515.86 1525.88 \n",
      "P2P=Disabled Latency Matrix (us)\n",
      "   GPU     0      1      2      3 \n",
      "     0   2.55  31.80  15.32  12.34 \n",
      "     1  18.86   2.68  18.60  11.85 \n",
      "     2  18.43  12.41   2.70  17.79 \n",
      "     3  11.71  16.34  18.25   2.48 \n",
      "\n",
      "   CPU     0      1      2      3 \n",
      "     0   2.32   6.96   6.25   7.09 \n",
      "     1   6.95   2.28   6.77   6.21 \n",
      "     2   6.53   6.44   2.01   6.15 \n",
      "     3   7.15   6.42   5.71   2.03 \n",
      "P2P=Enabled Latency (P2P Writes) Matrix (us)\n",
      "   GPU     0      1      2      3 \n",
      "     0   2.55   2.29  13.68  20.24 \n",
      "     1   2.23   2.67  20.34  20.34 \n",
      "     2  16.72  16.87   2.71   2.30 \n",
      "     3  16.22  16.19   2.30   2.47 \n",
      "\n",
      "   CPU     0      1      2      3 \n",
      "     0   2.61   1.87   6.40   6.43 \n",
      "     1   2.11   2.64   7.10   6.34 \n",
      "     2   6.70   6.48   2.31   1.88 \n",
      "     3   7.31   6.54   1.71   2.58 \n",
      "\n",
      "NOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Tests on GPU pairs using P2P and without P2P \n",
    "#`git clone --depth 1 --branch v11.2 https://github.com/NVIDIA/cuda-samples.git`\n",
    "!/dli/cuda-samples/bin/x86_64/linux/release/p2pBandwidthLatencyTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.2 Basic SLURM Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen how GPUs can communicate with each other over NVLink, let's go over how the hardware resources can be organized into compute nodes. These nodes can be managed by Cluster Manager such as [*Slurm Workload Manager*](https://slurm.schedmd.com/), an open source cluster management and job scheduler system for large and small Linux clusters. \n",
    "\n",
    "\n",
    "For this lab, we have configured a SLURM manager where the 4 available GPUs are partitioned into 2 nodes: **slurmnode1** \n",
    "and **slurmnode2**, each with 2 GPUs. \n",
    "\n",
    "Next, let's see some basic SLURM commands. More SLURM commands can be found in the [SLURM official documentation](https://slurm.schedmd.com/).\n",
    "\n",
    "<img src=\"images/cluster_overview.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.1 Check the SLURM Configuration\n",
    "\n",
    "We can check the available resources in the SLURM cluster by running `sinfo`. The output will show that there are 2 nodes in the cluster **slurmnode1** and **slurmnode2**. Both nodes are currently idle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\n",
      "slurmpar*    up   infinite      2   idle slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "# Check available resources in the cluster\n",
    "!sinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.2.2 Submit Jobs Using `srun` Command\n",
    "\n",
    "The `srun` command allows to running parallel jobs. \n",
    "\n",
    "The argument **-N** (or *--nodes*) can be used to specify the nodes allocated to a job. It is also possible to allocate a subset of GPUs available within a node by specifying the argument **-G (or --gpus)**.\n",
    "\n",
    "Check out the [SLURM official documentation](https://slurm.schedmd.com/) for more arguments.\n",
    "\n",
    "To test running parallel jobs, let's submit a job that requests 1 node (2 GPUs) and run a simple command on it: `nvidia-smi`. We should see the output of 2 GPUs available in the allocated node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 21 14:31:22 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              75W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              73W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# run nvidia-smi slurm job with 1 node allocation\n",
    "!srun -N 1 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's now allocate 2 nodes and run again `nvidia-smi` command.\n",
    "\n",
    "We should see the results of both nodes showing the available GPU devices. Notce that the stdout might be scrumbled due to the asynchronous and parallel execution of `nvidia-smi` command in the two nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 21 14:31:22 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              75W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  | 00000002:00:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              73W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "Thu Mar 21 14:31:23 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000003:00:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              54W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          On  | 00000004:00:00.0 Off |                    0 |\n",
      "| N/A   41C    P0              59W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# run nvidia-smi slurm job with 2 node allocation.\n",
    "!srun -N 2 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3 Submit Jobs Using `sbatch` Command \n",
    "\n",
    "In the previous examples, we allocated resources to run one single command. For more complex jobs, the `sbatch` command allows submitting batch scripts to SLURM by specifying the resources and all environment variables required for executing the job. `sbatch` will transfer the execution to the SLURM Manager after automatically populating the arguments.\n",
    "\n",
    "In the batch script below, `#SBATCH ...` is used to specify resources and other options relating to the job to be executed:\n",
    "\n",
    "```\n",
    "        #!/bin/bash\n",
    "        #SBATCH -N 1                               # Node count to be allocated for the job\n",
    "        #SBATCH --job-name=dli_firstSlurmJob       # Job name\n",
    "        #SBATCH -o /dli/nemo/logs/%j.out       # Outputs log file \n",
    "        #SBATCH -e /dli/nemo/logs/%j.err       # Errors log file\n",
    "\n",
    "        srun -l my_script.sh                       # my SLURM script \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we submit the `sbatch` batch script, let's first prepare a job that will be executed: a short batch script that will sleep for 2 seconds before running the `nvidia-smi` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "sleep 2\n",
      "nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "!chmod +x /dli/code/test.sh # \"chmod\" modifies the permissions and access mode of files and directories\n",
    "                            # +x means that the file is executable\n",
    "# Check the batch script \n",
    "!cat /dli/code/test.sh # uses \"cat\" command to display file contents\n",
    "                       # \"!/bin/bash - \"tells your terminal that when you run the script it should use bash to execute it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit this batch script job, let's create an `sbatch` script that initiates the resources to be allocated and submits the test.sh job.\n",
    "\n",
    "The following cell will edit the `test_sbatch.sbatch` script allocating 1 node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The image you've provided appears to be a screenshot of a Jupyter notebook explaining how to submit jobs using the `sbatch` command in a High-Performance Computing (HPC) environment managed by the SLURM workload manager. Here's a breakdown of what's happening:\n",
    "\n",
    "1. **sbatch**: The `sbatch` command is used to submit a job to the SLURM scheduler. This job will be queued and executed when the resources specified in the `sbatch` script become available. \n",
    "\n",
    "2. **SLURM**: The Simple Linux Utility for Resource Management (SLURM) is an open-source, fault-tolerant, and highly scalable cluster management and job scheduling system for large and small Linux clusters.\n",
    "\n",
    "3. **Batch Script**: The text shown is a batch script that is written for the SLURM scheduler. This script contains several `#SBATCH` directives which specify job settings like the number of nodes, job name, output, and error log paths.\n",
    "\n",
    "   - `#SBATCH -N 1` indicates that the job should use one compute node.\n",
    "   - `#SBATCH --job-name=dl_firstSlurmJob` sets the job name to `dl_firstSlurmJob`.\n",
    "   - `#SBATCH -o /dl1/nemo/logs/%x.%j.out` specifies where to write the standard output of the job. `%x` is the job name and `%j` is the job ID.\n",
    "   - `#SBATCH -e /dl1/nemo/logs/%x.%j.err` specifies where to write the standard error output of the job.\n",
    "\n",
    "4. **Script Execution**: The command `srun -l my_script.sh` at the end of the batch script will be executed on the allocated node(s). This script is likely to contain the actual commands that need to be run as part of the job. The `-l` option prefixes each line of output with the task number.\n",
    "\n",
    "5. **Job Preparation**: Before the job can be submitted, the notebook instructs the user to make a script executable (`chmod +x /dl1/code/test.sh`) and then provides a cell to check the contents of the script (`cat /dl1/code/test.sh`).\n",
    "\n",
    "6. **Job Submission**: The Jupyter notebook provides a cell to write the `test_sbatch.sbatch` script, which sets up the `sbatch` job configuration, and another cell is provided to inspect this script using the `cat` command.\n",
    "\n",
    "7. **Final Submission**: The last step would be to actually submit the job to the SLURM scheduler, which is not shown in the screenshot but would typically involve running a command like `sbatch test_sbatch.sbatch`.\n",
    "\n",
    "This process is typical for running batch jobs on an HPC cluster, where jobs are queued and managed by a scheduler to optimize the use of compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /dli/code/test_sbatch.sbatch\n"
     ]
    }
   ],
   "source": [
    "%%writefile /dli/code/test_sbatch.sbatch\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH -N 1\n",
    "#SBATCH --job-name=dli_firstSlurmJob\n",
    "#SBATCH -o /dli/nemo/logs/%j.out\n",
    "#SBATCH -e /dli/nemo/logs/%j.err\n",
    "\n",
    "srun -l /dli/code/test.sh  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#SBATCH -N 1\n",
      "#SBATCH --job-name=dli_firstSlurmJob\n",
      "#SBATCH -o /dli/nemo/logs/%j.out\n",
      "#SBATCH -e /dli/nemo/logs/%j.err\n",
      "\n",
      "srun -l /dli/code/test.sh  \n"
     ]
    }
   ],
   "source": [
    "# Check the sbatch script \n",
    "!cat /dli/code/test_sbatch.sbatch # \"cat\" command can be used to display the content of a file, copy content from one file to another, concatenate the contents of multiple files, display \n",
    "                                  # the line number, display $ at the end of the line, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's submit the `sbatch` job and check the SLURM scheduler. The batch script will be queued and executed when the requested resources are available.\n",
    "\n",
    "The `squeue` command shows the running or pending jobs. An output example is shown below: \n",
    "\n",
    "```\n",
    "Submitted batch job **\n",
    "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
    "                **  slurmpar test_sba    admin  R       0:01      1 slurmnode1\n",
    "\n",
    "```\n",
    "\n",
    "It shows the SLURM Job ID, Job's name, the user ID, Job's Status (R=running), running duration and the allocated node name.\n",
    "\n",
    "The following cell submits the `sbatch` job, collects the `JOBID` variable (for querying later the logs) and checks the jobs in the SLURM scheduling queue.\n",
    "\n",
    "---\n",
    "\n",
    "The `JOBID` in a SLURM managed cluster is not reset to 1 upon restarting your kernel or even the SLURM service itself. It is a sequentially increasing number managed by the SLURM controller that uniquely identifies each job submitted to the queue. This is intentional for several reasons:\n",
    "\n",
    "1. **Uniqueness**: Each job must have a unique identifier so that there is no ambiguity when referring to jobs, especially in a multi-user environment where many jobs may be submitted, running, or have completed.\n",
    "   \n",
    "2. **Accounting and Logging**: SLURM keeps detailed accounting logs that track resource usage, job status, and other metadata associated with job execution. Using a continuously incrementing `JOBID` helps maintain accurate and consistent records across system reboots and service restarts.\n",
    "\n",
    "3. **Resilience**: By not resetting the `JOBID` counter, the system avoids the risk of job ID collisions, which could potentially cause issues with job dependencies, accounting, and tracking.\n",
    "\n",
    "The `JOBID` is designed to be a system-wide counter and not a session or user-specific counter. This is why when you restart your Jupyter kernel, which is essentially restarting your Python session, the `JOBID` does not reset. The SLURM controller, which assigns these IDs, operates independently of your individual session and maintains its state across individual job submissions and system reboots until it is explicitly reset by an administrator, or it rolls over due to reaching the maximum value.\n",
    "\n",
    "---\n",
    "\n",
    "In a Jupyter Notebook, lines starting with an exclamation mark `!` execute shell commands in the underlying operating system. The snippet you've provided is combining shell commands with Python code to interact with the SLURM job scheduling system.\n",
    "\n",
    "Here's what each part of the snippet does:\n",
    "\n",
    "1. `JOBID=!squeue -u root | grep dli | awk '{print $1}'`: This line performs several actions:\n",
    "   - `squeue -u root`: This is a SLURM command that lists all jobs queued or running for the user `root`.\n",
    "   - `| grep dli`: The `grep` command filters the output from `squeue` to only include lines that contain the string \"dli\", which likely corresponds to the job name or user associated with the jobs you're interested in.\n",
    "   - `| awk '{print $1}'`: This part of the command uses `awk`, a powerful text-processing tool, to print only the first field of each line, which in the output of `squeue` corresponds to the job ID.\n",
    "   - The entire command is preceded by `!`, which tells the Jupyter Notebook to run the command in the shell, and the result is assigned to the variable `JOBID`.\n",
    "\n",
    "2. `slurm_job_output='/dli/nemo/logs/'+JOBID[0]+'.out'`: This line is constructing a file path for the log file associated with the job ID obtained in the previous step. In Python, `JOBID[0]` would refer to the first element of a list named `JOBID`. Since the previous command should return a list with a single string (the job ID), `JOBID[0]` gets that job ID.\n",
    "\n",
    "   - `/dli/nemo/logs/`: This is the directory path where SLURM log files are stored.\n",
    "   - `JOBID[0]`: This is the job ID extracted from the previous command.\n",
    "   - `'.out'`: This is the file extension for the standard output log file. When combined, this creates a path to the SLURM log file for the given job ID.\n",
    "\n",
    "Together, these commands are used to identify the log file associated with a SLURM job and to construct the path to that log file so that it can be accessed later, possibly to check the output of the job or to diagnose any issues that occurred during its execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 24\n",
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                24  slurmpar dli_firs     root PD       0:00      1 (None)\n"
     ]
    }
   ],
   "source": [
    "# Submit the job\n",
    "!sbatch /dli/code/test_sbatch.sbatch\n",
    "\n",
    "# Get the JOBID variable\n",
    "JOBID=!squeue -u root | grep dli | awk '{print $1}'\n",
    "slurm_job_output='/dli/nemo/logs/'+JOBID[0]+'.out'\n",
    "\n",
    "\"\"\"\n",
    "Grep, or global regular expression print, is one of the most versatile and useful Linux commands available. It searches for text and strings defined by users in a given file. \n",
    "Grep allows users to search files for a specific pattern or word and see which lines contain it.\n",
    "\n",
    "The awk command's main purpose is to make information retrieval and text manipulation easy to perform in Linux. This Linux command works by scanning a set of input lines in \n",
    "order and searches for lines matching the patterns specified by the user.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# check the jobs in the SLURM scheduling queue\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output log file for the executed job (**JOBID.out**) is automatically created to gather the outputs.\n",
    "\n",
    "In our case, we should see the results of `nvidia-smi` command that was executed in the `test.sh` script submitted with 1 node allocation. Let's have a look at execution logs:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Thu Mar 21 14:31:28 2024       \n",
      "0: +---------------------------------------------------------------------------------------+\n",
      "0: | NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "0: |-----------------------------------------+----------------------+----------------------+\n",
      "0: | GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "0: | Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "0: |                                         |                      |               MIG M. |\n",
      "0: |=========================================+======================+======================|\n",
      "0: |   0  NVIDIA A100 80GB PCIe          On  | 00000001:00:00.0 Off |                    0 |\n",
      "0: | N/A   38C    P0              55W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "0: |                                         |                      |             Disabled |\n",
      "0: +-----------------------------------------+----------------------+----------------------+\n",
      "0: |   1  NVIDIA A100 80GB PCIe          On  | 00000002:00:00.0 Off |                    0 |\n",
      "0: | N/A   37C    P0              54W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "0: |                                         |                      |             Disabled |\n",
      "0: +-----------------------------------------+----------------------+----------------------+\n",
      "0:                                                                                          \n",
      "0: +---------------------------------------------------------------------------------------+\n",
      "0: | Processes:                                                                            |\n",
      "0: |  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "0: |        ID   ID                                                             Usage      |\n",
      "0: |=======================================================================================|\n",
      "0: |  No running processes found                                                           |\n",
      "0: +---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Wait 3 seconds to let the job execute and get the populated logs \n",
    "!sleep 3\n",
    "\n",
    "# Check the execution logs \n",
    "!cat $slurm_job_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4  Exercise: Submit Jobs Using `sbatch` Command  Requesting More Resources\n",
    "\n",
    "\n",
    "Using what you have learned, submit the previous `test.sh` batch script with the `sbatch` command on **2 nodes** allocation.\n",
    "\n",
    "To do so, you will need to:\n",
    "1. Modify the `test_sbatch.sbatch` script to allocate 2 Nodes \n",
    "2. Submit the script again using `sbatch` command\n",
    "3. Check the execution logs \n",
    "\n",
    "\n",
    "If you get stuck, you can look at the [solution](solutions/ex1.2.4.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Linux command \"/bin/bash\" - tells your terminal that when you run the script it should use bash to execute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\n",
      "slurmpar*    up   infinite      2   idle slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "# 1. Modify the `test_sbatch.sbatch` script to allocate 2 Nodes\n",
    "!sinfo\n",
    "#!srun -N 2 nvidia-smi\n",
    "\n",
    "%%writefile /dli/code/test_sbatch.sbatch \n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH -N 2\n",
    "#SBATCH --job-name=dli_2nodes\n",
    "#SBATCH -o /dli/nemo/log_2nodes/%j.out\n",
    "#SBATCH -e /dli/nemo/log_2nodes/%j.err\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Submit the script again using `sbatch` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Check the execution logs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1.3 Run Interactive Sessions \n",
    "\n",
    "Interactive sessions allow to connect directly to a worker node and interact with it through the terminal. \n",
    "\n",
    "The SLURM manager allows to allocate resources in interactive session using the `--pty` argument as follows: `srun -N 1 --pty /bin/bash`. \n",
    "The session is closed when you exit the node or you cancel the interactive session job using the command `scancel JOBID`.\n",
    "\n",
    "\n",
    "Since this is an interactive session, first, we need to launch a terminal window and submit a slurm job allocating resources in interactive mode. To do so, we will need to follow the 3 steps: \n",
    "1. Launch a terminal session\n",
    "2. Check the GPUs resources using the command `nvidia-smi` \n",
    "3. Run an interactive session requesting 1 node by executing `srun -N 1 --pty /bin/bash`\n",
    "4. Check the GPUs resources using the command `nvidia-smi` again \n",
    "\n",
    "Let's run our first interactive job requesting 1 node and check what GPU resources are at our disposal. \n",
    "\n",
    "![title](images/interactive_launch.png)\n",
    "\n",
    "Notice that while connected to the session, the host name as displayed in the command line changes from \"lab\" (login node name) to \"slurmnode1\" indicating that we are now successfully working on a remote worker node.\n",
    "\n",
    "Run the following cell to get a link to open a terminal session and the instructions to run an interactive session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<pre>\n",
       "   Step 1: Open a terminal session by following this <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
       "   Step 2: Check the GPUs resources: <font color=\"green\">nvidia-smi</font>\n",
       "   Step 3: Run an interactive session: <font color=\"green\">srun -N 1 --pty /bin/bash</font>\n",
       "   Step 4: Check the GPUs resources again: <font color=\"green\">nvidia-smi</font>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<pre>\n",
    "   Step 1: Open a terminal session by following this <a href=\"\", data-commandlinker-command=\"terminal:create-new\">Terminal link</a>\n",
    "   Step 2: Check the GPUs resources: <font color=\"green\">nvidia-smi</font>\n",
    "   Step 3: Run an interactive session: <font color=\"green\">srun -N 1 --pty /bin/bash</font>\n",
    "   Step 4: Check the GPUs resources again: <font color=\"green\">nvidia-smi</font>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "You've made it through the first section of the course and are ready to begin training Deep Learning models on multiple GPUs. <br>\n",
    "\n",
    "Before moving on, we need to make sure that no jobs are still running or waiting on the SLURM queue. \n",
    "Let's check the SLURM jobs queue by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the user's jobs using the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Cancel admin user jobs\n",
    "!scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will be running basic GPT language model training on different distribution configurations. Move on to [02_GPT_LM_pretrainings.ipynb](02_GPT_LM_pretrainings.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
